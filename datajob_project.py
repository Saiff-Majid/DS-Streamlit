# -*- coding: utf-8 -*-
"""Datajob project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hd3UK1_Spo5tDwm-LQHykLLekurnDu6w

**Datajob Project**

2020-2022 Kaggle Machine Learning & Data Science Survey

By:

Saif MAJID

Richard Dooso

Rachel Hill

# **1. Load, explore and Merge the Datasets**
"""

#imports
import pandas as pd
import plotly.express as px
from google.colab import files
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
from collections import defaultdict
from google.colab import drive
import gdown
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.figure_factory as ff

# Define file IDs and years
file_ids = {
    "2020": '1KGyT_F0MPIcTapjXjWLJoezJU9FZhoPm',
    "2021": '19uDNeaq76cs1T3EODWwCzNfnJ4xy4p5H',
    "2022": '17kp8UTXwaaHzsnRkkEljRjnTQ5Y_o68C'
}

# Download each file and load into DataFrames
df_2020 = None
df_2021 = None
df_2022 = None

# Load each dataset into a corresponding variable
for year, file_id in file_ids.items():
    url = f'https://drive.google.com/uc?id={file_id}'
    gdown.download(url, f'survey_{year}.csv', quiet=False)

    # Load the dataset and name it based on the year
    if year == "2020":
        df_2020 = pd.read_csv(f'survey_{year}.csv', header=1, low_memory=False)
    elif year == "2021":
        df_2021 = pd.read_csv(f'survey_{year}.csv', header=1, low_memory=False)
    elif year == "2022":
        df_2022 = pd.read_csv(f'survey_{year}.csv', header=1, low_memory=False)

df_2020.head()

df_2021.head()

df_2022.head()

# Function to extract the base question using '-' as a separator
def extract_base_question(question):
    return question.split('-')[0].strip()

# Function to get unique questions in the same order as they appear in the dataset
def get_unique_ordered_questions(columns):
    seen = set()
    unique_questions = []
    for col in columns:
        base_question = extract_base_question(col)
        if base_question not in seen:
            seen.add(base_question)
            unique_questions.append(base_question)
    return unique_questions


# Extract unique base questions in the same order for each dataset
unique_questions_2020 = get_unique_ordered_questions(df_2020.columns)
unique_questions_2021 = get_unique_ordered_questions(df_2021.columns)
unique_questions_2022 = get_unique_ordered_questions(df_2022.columns)

# Print the number of unique base questions for each dataset
print(f"Number of unique base questions in 2020 dataset: {len(unique_questions_2020)}")
print(f"Number of unique base questions in 2021 dataset: {len(unique_questions_2021)}")
print(f"Number of unique base questions in 2022 dataset: {len(unique_questions_2022)}")

# Print the unique base questions with a count for each dataset
def print_questions_with_count(questions, dataset_year):
    print(f"\nUnique base questions for {dataset_year} dataset:")
    for idx, question in enumerate(questions, start=1):
        print(f"Q{idx}: {question}")

print_questions_with_count(unique_questions_2020, "2020")
print_questions_with_count(unique_questions_2021, "2021")
print_questions_with_count(unique_questions_2022, "2022")

#Convvert unique questions into sets
unique_set_2020 = set(unique_questions_2020)
unique_set_2021 = set(unique_questions_2021)
unique_set_2022 = set(unique_questions_2022)

# Find the intersection of the three sets to get common questions across all datasets
common_questions_all = unique_set_2022.intersection(unique_set_2021, unique_set_2020)

# Output the number of common questions and list them
print(f"Number of questions common across all three datasets: {len(common_questions_all)}")

# Print the common questions
print("Common questions across all three datasets:")
for idx, question in enumerate(common_questions_all, start=1):
    print(f"Q{idx}: {question}")

# Add a 'Year' column to each dataset to indicate the source year
df_2020['Year'] = 2020
df_2021['Year'] = 2021
df_2022['Year'] = 2022

# Identify all columns related to the common questions across datasets
def get_all_variants(df, common_questions):
    variants = []
    for col in df.columns:
        base_question = extract_base_question(col)
        if base_question in common_questions:
            variants.append(col)
    return variants

# Filter each dataset to include only the columns related to common questions, plus the 'Year' column
columns_2020 = get_all_variants(df_2020, common_questions_all) + ['Year']
columns_2021 = get_all_variants(df_2021, common_questions_all) + ['Year']
columns_2022 = get_all_variants(df_2022, common_questions_all) + ['Year']

filtered_2020 = df_2020[columns_2020]
filtered_2021 = df_2021[columns_2021]
filtered_2022 = df_2022[columns_2022]

# Concatenate the filtered datasets to create a full survey dataset with all variants of common questions
df = None
df = pd.concat([filtered_2020, filtered_2021, filtered_2022], ignore_index=True)


df

#code to compare the ditribution of the dataset across the three years
year_column = "Year"

# Specifying the years to be counted
year_2020 = 2020
year_2021 = 2021
year_2022 = 2022

# Count entries for each year
count_2020 = df[df[year_column] == year_2020].shape[0]
count_2021 = df[df[year_column] == year_2021].shape[0]
count_2022 = df[df[year_column] == year_2022].shape[0]

# Print the results
print(f"Year {year_2020}: {count_2020} entries")
print(f"Year {year_2021}: {count_2021} entries")
print(f"Year {year_2022}: {count_2022} entries")

#Some answers got stranded at the end. Creating variables with a filtered list of matching questions.

programming_cols = [col for col in df.columns if "What programming languages" in col]
IDE_cols = [col for col in df.columns if "integrated development environments (IDE's)" in col]
ML_cols = [col for col in df.columns if " following machine learning frameworks" in col]
mlAl_cols = [col for col in df.columns if "following ML algorithms" in col]
compvis_cols = [col for col in df.columns if "categories of computer vision methods" in col]
NLP_cols = [col for col in df.columns if "following natural language processing (NLP) methods" in col]
workrole_cols = [col for col in df.columns if "important part of your role at work" in col]
platform_cols = [col for col in df.columns if "platforms have you begun or completed data science courses" in col]
media_cols = [col for col in df.columns if "favorite media sources" in col]

#Create a variable to store the same order to these columns as we extracted from the dataset
ordered_columns = programming_cols + IDE_cols + ML_cols + mlAl_cols + compvis_cols + NLP_cols + workrole_cols + platform_cols + media_cols

#create another variable for ordering and storing the non-multiple choice questions
cols_to_move = ["Year", "What is your age (# years)?", "What is your gender? - Selected Choice", "In which country do you currently reside?",
                "What is the highest level of formal education that you have attained or plan to attain within the next 2 years?",
                "Select the title most similar to your current role (or most recent title if retired): - Selected Choice",
                "For how many years have you been writing code and/or programming?", "Approximately how many times have you used a TPU (tensor processing unit)?",
                "For how many years have you used machine learning methods?", "What is the size of the company where you are employed?",
                "Approximately how many individuals are responsible for data science workloads at your place of business?", "Does your current employer incorporate machine learning methods into their business?",
                "What is your current yearly compensation (approximate $USD)?"]

#combine the two variables to reorder the dataset
df = df[cols_to_move + ordered_columns]

df.head()

for index, question in enumerate(df.columns):
    print(f"Column {index }: {question}")

"""## **2. Data Cleaning and Preprocessing**

**Column Selection: Since we're primarily focusing on technical roles :**
- Column 0 (Year)
- Column 1 (Age)
- Column 2 (Gender)
- Column 3 (Country)
- Column 4 (Education)
- Column 5 (Role Title)
- Column 6 (Programming Experience)
- Column 8 (ML Experience)
- Column 9 (Company Size)
- Column 12 (Salary)
- Column 13 to 28 (Programming Languages)
- Column 29 to 45 (IDEs)
- Column 46 to 63 (ML Frameworks)
- Column 64 to 77 (ML Algorithms)
- Column 110 to 121 (Learning Platforms)
"""

# Initialize df_cleaned as an empty DataFrame
df_cleaned = pd.DataFrame()

### 1. Role Title Column ###
df = df.dropna(subset=[df.columns[5]])
role_map = {
    'Student': 'Student',
    'Data Scientist': 'Data Scientist',
    'Software Engineer': 'Software Engineer',
    'Currently not employed': 'Unemployed',
    'Other': 'Other',
    'Data Analyst': 'Data Analyst',
    'Data Analyst (Business, Marketing, Financial, Quantitative, etc)': 'Data Analyst',
    'Research Scientist': 'Research Scientist',
    'Machine Learning Engineer': 'Machine Learning Engineer',
    'Machine Learning/ MLops Engineer': 'Machine Learning Engineer',
    'Business Analyst': 'Business Analyst',
    'Data Engineer': 'Data Engineer',
    'Program/Project Manager': 'Project Manager',
    'Product/Project Manager': 'Project Manager',
    'Manager (Program, Project, Operations, Executive-level, etc)': 'Manager',
    'Engineer (non-software)': 'Engineer',
    'Teacher / professor': 'Teacher/Professor',
    'Statistician': 'Statistician',
    'Product Manager': 'Product Manager',
    'DBA/Database Engineer': 'Database Engineer',
    'Developer Relations/Advocacy': 'Developer Advocate',
    'Developer Advocate': 'Developer Advocate',
    'Data Architect': 'Data Architect',
    'Data Administrator': 'Data Administrator'
}
df_cleaned['Role_Title'] = df.iloc[:, 5].replace(role_map)

### 2. Programming Languages (Columns 13-28) ###
language_columns = df.columns[13:28]
language_data = {}
for col in language_columns:
    language_name = "Language - " + df[col].name.split(" - ")[-1].strip()
    language_data[language_name] = df[col].apply(lambda x: 1 if pd.notna(x) else 0)
language_df = pd.DataFrame(language_data)
df_cleaned = pd.concat([df_cleaned, language_df], axis=1)
df_cleaned.drop(columns=["Language - None"], inplace=True, errors='ignore')

### 3. IDEs (Columns 29-45) ###
ide_columns = df.columns[29:45]
ide_data = {}
for col in ide_columns:
    ide_name = "IDE - " + df[col].name.split(" - ")[-1].strip()
    ide_data[ide_name] = df[col].apply(lambda x: 1 if pd.notna(x) else 0)
ide_df = pd.DataFrame(ide_data)
df_cleaned = pd.concat([df_cleaned, ide_df], axis=1)
df_cleaned.drop(columns=["IDE - None"], inplace=True, errors='ignore')

### 4. ML Frameworks (Columns 46-63) ###
ml_framework_columns = df.columns[46:63]
framework_data = {}
for col in ml_framework_columns:
    framework_name = "Framework - " + df[col].name.split(" - ")[-1].strip()
    framework_data[framework_name] = df[col].apply(lambda x: 1 if pd.notna(x) else 0)
framework_df = pd.DataFrame(framework_data)
df_cleaned = pd.concat([df_cleaned, framework_df], axis=1)
df_cleaned.drop(columns=["Framework - None"], inplace=True, errors='ignore')

### 5. ML Algorithms (Columns 64-77) ###
ml_algorithm_columns = df.columns[64:77]
algorithm_data = {}
for col in ml_algorithm_columns:
    algorithm_name = "Algorithm - " + df[col].name.split(" - ")[-1].strip()
    algorithm_data[algorithm_name] = df[col].apply(lambda x: 1 if pd.notna(x) else 0)
algorithm_df = pd.DataFrame(algorithm_data)
df_cleaned = pd.concat([df_cleaned, algorithm_df], axis=1)
df_cleaned.drop(columns=["Algorithm - None"], inplace=True, errors='ignore')

### 6. Learning PLatforms (Columns 110-121) ###
learning_pl_columns = df.columns[110:121]
learning_data = {}
for col in learning_pl_columns:
    learning_name = "Learning - " + df[col].name.split(" - ")[-1].strip()
    learning_data[learning_name] = df[col].apply(lambda x: 1 if pd.notna(x) else 0)
learning_df = pd.DataFrame(learning_data)
df_cleaned = pd.concat([df_cleaned, learning_df], axis=1)
df_cleaned.drop(columns=["Learning - None"], inplace=True, errors='ignore')

### 7. Salary Column - Split into Min and Max ###
def extract_salary_min_max(salary):
    if pd.isna(salary):
        return None, None
    elif '>' in salary:
        # Assign high estimates for ">$500,000" or similar
        return 500000, None if salary == "> $500,000" else 1000000
    else:
        salary = re.sub(r'[>$,]', '', salary)
        if '-' in salary:
            low, high = salary.split('-')
            return int(low), int(high)
        else:
            return int(salary), int(salary)

# Apply function and add two columns to df_cleaned for salary min and max
df_cleaned[['Salary_Min', 'Salary_Max']] = df.iloc[:, 12].apply(lambda x: pd.Series(extract_salary_min_max(x)))

# Add Average Salary
df_cleaned['Average_Salary'] = df_cleaned.apply(
    lambda row: (row['Salary_Min'] + row['Salary_Max']) / 2 if pd.notna(row['Salary_Min']) and pd.notna(row['Salary_Max']) else None,
    axis=1
)

# handling missing values in Salary_Min and Salary_Max
# df_cleaned[['Salary_Min', 'Salary_Max']] = df_cleaned[['Salary_Min', 'Salary_Max']].fillna('Unknown')
# df_cleaned[['Salary_Min', 'Salary_Max']] = df_cleaned[['Salary_Min', 'Salary_Max']].fillna(df_cleaned['Salary_Min'].median())
# df_cleaned.dropna(subset=['Salary_Min', 'Salary_Max'], inplace=True)

### 8. Adding Year and Country ###
df_cleaned['Year'] = df['Year']
df_cleaned['Country'] = df['In which country do you currently reside?']


### Column 9: Machine Learning Experience ###
# Mapping of experience ranges to their midpoints
ml_experience_map = {
    'Under 1 year': 0.5,
    '1-2 years': 1.5,
    '2-3 years': 2.5,
    '3-4 years': 3.5,
    '4-5 years': 4.5,
    '5-10 years': 7.5,
    '10-20 years': 15,
    '20 or more years': 20,
    'I do not use machine learning methods': 0
}

# Adding 'ML_Experience_Category' and 'ML_Experience_Midpoint' columns
df_cleaned['ML_Experience_Category'] = df.iloc[:, 8]
df_cleaned['ML_Experience_Midpoint'] = df_cleaned['ML_Experience_Category'].map(ml_experience_map)

# Drop rows with NaN in ML_Experience columns
df_cleaned = df_cleaned.dropna(subset=['ML_Experience_Category', 'ML_Experience_Midpoint'])

### Column 10: Programming Experience ###
# Mapping of experience ranges to their midpoints for programming
programming_experience_map = {
    '< 1 years': 0.5,
    '1-2 years': 1.5,
    '1-3 years': 2,
    '3-5 years': 4,
    '5-10 years': 7.5,
    '10-20 years': 15,
    '20+ years': 20,
    'I have never written code': 0
}

# Adding 'Programming_Experience_Category' and 'Programming_Experience_Midpoint' columns
df_cleaned['Programming_Experience_Category'] = df.iloc[:, 6]
df_cleaned['Programming_Experience_Midpoint'] = df_cleaned['Programming_Experience_Category'].map(programming_experience_map)

# Drop rows with NaN in Programming_Experience columns
df_cleaned = df_cleaned.dropna(subset=['Programming_Experience_Category', 'Programming_Experience_Midpoint'])

### 11. Company Size ###
# Define a mapping for company size to their midpoints
company_size_map = {
    '0-49 employees': 25,
    '50-249 employees': 150,
    '250-999 employees': 625,
    '1000-9,999 employees': 5500,
    '10,000 or more employees': 10000
}

# Calculate the median of the known company sizes
company_size_median = df.iloc[:, 75].dropna().map(company_size_map).median()

# Add "Unknown Company Size" with the calculated median to the mapping
company_size_map['Unknown Company Size'] = company_size_median

# Fill missing values with "Unknown Company Size" placeholder
df['Company_Size'] = df.iloc[:, 9].fillna("Unknown Company Size")

# Map company size to midpoints in df_cleaned
df_cleaned['Company_Size'] = df['Company_Size'].replace(company_size_map)

### 12. Age Column ###
# Define a mapping of age ranges to their midpoints
age_map = {
    '18-21': 19.5,
    '22-24': 23,
    '25-29': 27,
    '30-34': 32,
    '35-39': 37,
    '40-44': 42,
    '45-49': 47,
    '50-54': 52,
    '55-59': 57,
    '60-69': 64.5,
    '70+': 70
}

# Keep Age_Category as it is and add Age_Midpoint with mapped values
df_cleaned['Age_Category'] = df.iloc[:, 1]
df_cleaned['Age_Midpoint'] = df_cleaned['Age_Category'].map(age_map)

# Fill any missing Age_Midpoint with the median of the available midpoints
#df_cleaned['Age_Midpoint'].fillna(df_cleaned['Age_Midpoint'].median(), inplace=True)

### 13. Gender Column ###
gender_map = {
    'Man': 'Male',
    'Woman': 'Female',
    'Prefer not to say': 'Other',
    'Nonbinary': 'Other',
    'Prefer to self-describe': 'Other'
}
df_cleaned['Gender'] = df.iloc[:, 2].map(gender_map)


### 14. Education Column ###
# Define a mapping for education levels
education_map = {
    'Doctoral degree': 'PhD',
    'Master’s degree': 'Master',
    'Bachelor’s degree': 'Bachelor',
    'No formal education past high school': 'High School',
    'Some college/university study without earning a bachelor’s degree': 'Some College',
    'Professional degree': 'Professional',
    'Professional doctorate': 'Professional Doctorate',
    'I prefer not to answer': 'Unknown'
}

# Map column 4 to standardized education levels
df_cleaned['Education'] = df.iloc[:, 4].map(education_map)

# Handle any remaining missing values
df_cleaned['Education'] = df_cleaned['Education'].fillna('Unknown')



# Display the first few rows of the cleaned DataFrame
df_cleaned.head()

df_cleaned.info()

# Count entries for each year
count_2020 = df_cleaned[df_cleaned[year_column] == year_2020].shape[0]
count_2021 = df_cleaned[df_cleaned[year_column] == year_2021].shape[0]
count_2022 = df_cleaned[df_cleaned[year_column] == year_2022].shape[0]

# Print the results
print(f"Year {year_2020}: {count_2020} entries")
print(f"Year {year_2021}: {count_2021} entries")
print(f"Year {year_2022}: {count_2022} entries")

df_cleaned_salary = pd.DataFrame()
df_cleaned_salary = df_cleaned.dropna(subset=['Salary_Min', 'Salary_Max']).copy()
df_cleaned_salary.info()

"""# 3. Visualizations"""

# Salary Min and Max distribution
fig = px.histogram(df_cleaned_salary, x="Salary_Min", nbins=20, title="Distribution of Minimum Salaries")
fig.update_xaxes(title="Minimum Salary (USD)")
fig.update_yaxes(title="Count")
fig.show()

fig = px.histogram(df_cleaned_salary, x="Salary_Max", nbins=20, title="Distribution of Maximum Salaries")
fig.update_xaxes(title="Maximum Salary (USD)")
fig.update_yaxes(title="Count")
fig.show()

# Find the common range for x-axis
min_salary = min(df_cleaned_salary["Salary_Min"].min(), df_cleaned_salary["Salary_Max"].min())
max_salary = max(df_cleaned_salary["Salary_Min"].max(), df_cleaned_salary["Salary_Max"].max())

# Salary_Min
fig_min = px.histogram(df_cleaned_salary, x="Salary_Min", nbins=20, title="Distribution of Minimum Salaries")
fig_min.update_xaxes(title="Salary (USD)", range=[min_salary, max_salary])  # Set common range
fig_min.update_yaxes(title="Count")
fig_min.show()

#  Salary_Max
fig_max = px.histogram(df_cleaned_salary, x="Salary_Max", nbins=20, title="Distribution of Maximum Salaries")
fig_max.update_xaxes(title="Salary (USD)", range=[min_salary, max_salary])  # Set common range
fig_max.update_yaxes(title="Count")
fig_max.show()

# Calculate average salary
df_cleaned_salary['Average_Salary'] = (df_cleaned_salary['Salary_Min'] + df_cleaned_salary['Salary_Max']) / 2

# Average salary by role
avg_salary_by_role = df_cleaned_salary.groupby('Role_Title')['Average_Salary'].mean().reset_index()
fig = px.bar(avg_salary_by_role, x='Role_Title', y='Average_Salary', title="Average Salary by Role")
fig.update_xaxes(title="Role Title", tickangle=45)
fig.update_yaxes(title="Average Salary (USD)")
fig.show()

# Calculate average salary by education level
avg_salary_by_education = df_cleaned_salary.groupby('Education')['Average_Salary'].mean().reset_index()

# Create the bar chart
fig = px.bar(
    avg_salary_by_education,
    x='Education',
    y='Average_Salary',
    title="Average Salary by Education Level"
)

# Update chart axes and appearance
fig.update_xaxes(title="Education Level", tickangle=45)
fig.update_yaxes(title="Average Salary (USD)")
fig.show()

platform_columns = ["Learning - Coursera", "Learning - edX", "Learning - Kaggle Learn Courses", "Learning - DataCamp", "Learning - Fast.ai",
                    "Learning - Udacity", "Learning - Udemy", "Learning - LinkedIn Learning",
                    "Learning - Cloud-certification programs (direct from AWS, Azure, GCP, or similar)", "Learning - University Courses (resulting in a university degree)"]

# Melt the DataFrame to long format
df_long = df_cleaned_salary.melt(id_vars=["Salary_Min"],
                                 value_vars=platform_columns,
                                 var_name="Learning_Platform",
                                 value_name="Used")

# Filter only rows where the platform was used (value is 1)
df_long = df_long[df_long["Used"] == 1]

# Group by learning platform and calculate the average salary
avg_salary_per_platform = df_long.groupby("Learning_Platform")["Salary_Min"].mean().reset_index()

# Plot
fig = px.bar(avg_salary_per_platform,
             x="Learning_Platform",
             y="Salary_Min",
             title="Average Salary by Learning Platform",
             labels={"Salary_Min": "Average Salary (USD)", "Learning_Platform": "Learning Platform"},
             color="Salary_Min",
             color_continuous_scale="Blues")

# Sort bars in descending order
fig.update_layout(xaxis={'categoryorder':'total descending'})
fig.show()

language_cols = [col for col in df_cleaned.columns if col.startswith("Language - ")]
language_counts = df_cleaned[language_cols].sum().sort_values(ascending=False)

fig = px.bar(x=language_counts.index, y=language_counts.values,
             labels={'x': 'Programming Languages', 'y': 'Count'},
             title="Popularity of Programming Languages")
fig.update_layout(xaxis_tickangle=-45)
fig.show()

#Initialize a list
distribution_data = []

# the distribution of languages for platforms
for platform in platform_columns:
    # Filter the rows where the platform is True
    platform_data = df_cleaned[df_cleaned[platform] == True]

    # Count the occurrences of each language column for this platform
    language_counts = platform_data[language_cols].sum()

    # Store the data for visualization
    for language, count in language_counts.items():
        distribution_data.append({
            'Learning Platform': platform,
            'Language': language,
            'Count': count})

# Convert the data into a DataFrame for easier visualization
distribution_df = pd.DataFrame(distribution_data)

# Create a bar plot for distribution of languages per platform
fig = px.bar(
    distribution_df,
    x='Language',
    y='Count',
    color='Learning Platform',
    facet_col='Learning Platform',  # Create separate subplots for each learning platform
    title="Language Distribution Across Learning Platforms",
    labels={'Count': 'Number of Learners', 'Language': 'Language'},
    barmode='stack')

# display the platform name (remove "Learning Platform")
for annotation in fig.layout.annotations:
    annotation.text = annotation.text.split(" - ")[-1]  # Remove the "Learning Platform" prefix

fig.update_layout(
    yaxis_title="Number of Learners",
    showlegend=True,
    height=700,
    width=1700,
    title_x=0.5)


fig.show()

# Initialize a list
distribution_data = []

# For each Role Title, get the distribution of Learning Platforms
for role in df_cleaned['Role_Title'].unique():

    # Filter the rows where the Role Title matches
    role_data = df_cleaned[df_cleaned['Role_Title'] == role]

    #  count the number of True values
    for platform in platform_columns:
        platform_count = role_data[platform].sum()

        # Store the data
        distribution_data.append({
            'Role Title': role,
            'Learning Platform': platform,
            'Count': platform_count })

# Convert the data into a DataFrame
distribution_df = pd.DataFrame(distribution_data)

# bar plot: distribution of Learning Platforms per Role Title
fig = px.bar(
    distribution_df,
    x='Learning Platform',
    y='Count',
    color='Role Title',
    title="Learning Platform Distribution for Each Role Title",
    labels={'Count': 'Number of Learners', 'Learning Platform': 'Learning Platform'},
    barmode='stack')

#
fig.update_layout(
    xaxis_title="Learning Platform",
    yaxis_title="Number of Learners",
    showlegend=True,
    xaxis_tickangle=-45,
    height=600,
    width=1200)


fig.show()

# Calculate median salary by country
median_salary_by_country = df_cleaned_salary.groupby("Country")["Salary_Min"].median().sort_values(ascending=False).head(20)

# Plot median salary by country using Plotly
fig = px.bar(median_salary_by_country, orientation='h', title="Top 20 Countries by Median Minimum Salary")
fig.update_layout(yaxis_title="Country", xaxis_title="Median Minimum Salary (USD)", height=600)
fig.show()

# Average salary by country
avg_salary_by_country = df_cleaned_salary.groupby('Country')['Average_Salary'].mean().reset_index()

# Plot
fig = px.choropleth(avg_salary_by_country, locations="Country", locationmode="country names",
                    color="Average_Salary", hover_name="Country",
                    title="Average Salary by Country",
                    color_continuous_scale=px.colors.sequential.Plasma)
fig.update_geos(showcoastlines=True, coastlinecolor="Black", showland=True, landcolor="lightgray")
fig.update_coloraxes(colorbar_title="Avg Salary (USD)")
fig.show()

# Filter framework columns and group by Role_Title
framework_columns = [col for col in df_cleaned_salary.columns if "Framework - " in col]
role_framework_usage = df_cleaned_salary.groupby('Role_Title')[framework_columns].sum()

# Plot as heatmap
fig = px.imshow(role_framework_usage, title="ML Framework Usage by Role", labels=dict(x="Framework", y="Role Title", color="Usage Count"))
fig.update_xaxes(tickangle=45)
fig.show()

fig = px.box(df_cleaned_salary, x="Role_Title", y="Age_Midpoint", title="Age Distribution by Role")
fig.update_xaxes(title="Role Title", tickangle=45)
fig.update_yaxes(title="Age")
fig.show()

# Scatter plot: ML Experience vs Salary_Min
fig = px.scatter(
    df_cleaned,
    x='ML_Experience_Midpoint',
    y='Salary_Min',
    color='Role_Title',
    hover_data=['Role_Title', 'Country'],
    title="Machine Learning Experience vs. Minimum Salary",
    labels={'ML_Experience_Midpoint': 'ML Experience (Years)', 'Salary_Min': 'Minimum Salary (USD)'}
)
fig.update_layout(xaxis_title="ML Experience (Years)", yaxis_title="Minimum Salary (USD)")
fig.show()

# Scatter plot: Programming Experience vs Salary_Max
fig = px.scatter(
    df_cleaned,
    x='Programming_Experience_Midpoint',
    y='Salary_Max',
    color='Role_Title',
    hover_data=['Role_Title', 'Country'],
    title="Programming Experience vs. Maximum Salary",
    labels={'Programming_Experience_Midpoint': 'Programming Experience (Years)', 'Salary_Max': 'Maximum Salary (USD)'}
)
fig.update_layout(xaxis_title="Programming Experience (Years)", yaxis_title="Maximum Salary (USD)")
fig.show()

# Histogram: ML Experience vs Salary_Min
fig = px.histogram(
    df_cleaned,
    x='Salary_Min',
    color='Role_Title',
    nbins=30,
    title="Distribution of Minimum Salary by Role Title",
    labels={'Salary_Min': 'Minimum Salary (USD)', 'Role_Title': 'Role Title'},
    opacity=0.75  # Make the bars slightly transparent
)

fig.update_layout(
    xaxis_title="Minimum Salary (USD)",
    yaxis_title="Count of Roles"
)
fig.show()


# Histogram: Programming Experience vs Salary_Max
fig = px.histogram(
    df_cleaned,
    x='Salary_Max',
    color='Role_Title',
    nbins=30,
    title="Distribution of Maximum Salary by Role Title",
    labels={'Salary_Max': 'Maximum Salary (USD)', 'Role_Title': 'Role Title'},
    opacity=0.75)

fig.update_layout(
    xaxis_title="Maximum Salary (USD)",
    yaxis_title="Count of Roles")

fig.show()

# Replace outliers (salaries > 500000) with the average salary for each country
df_cleaned_salary['Average_Salary'] = (df_cleaned_salary['Salary_Min'] + df_cleaned_salary['Salary_Max']) / 2
df_cleaned_salary['Adjusted_Salary'] = df_cleaned_salary['Average_Salary']

# Calculate mean salary for each country
mean_salaries_by_country = df_cleaned_salary.groupby('Country')['Average_Salary'].mean()

# Replace salaries over 500000 with the mean salary of each country
df_cleaned_salary['Adjusted_Salary'] = df_cleaned_salary.apply(
    lambda row: mean_salaries_by_country[row['Country']] if row['Adjusted_Salary'] > 500000 else row['Adjusted_Salary'],
    axis=1
)

# Plotting salary distribution by country using plotly
fig = px.box(df_cleaned_salary, y='Country', x='Adjusted_Salary',
             title="Salary Distribution by Country (Outliers Replaced by Mean)",
             color='Country',
             points="all",
             width=2000,
             height=2000,
             hover_data=['Adjusted_Salary'])

fig.update_layout(showlegend=True, xaxis_title="Adjusted Salary (USD)", yaxis_title="Country")
fig.show()

# Violin plot: ML Experience by Role Title
fig = px.violin(
    df_cleaned,
    x='Role_Title',
    y='ML_Experience_Midpoint',
    box=True,
    points="all",
    color='Role_Title',
    title="Machine Learning Experience Distribution by Role Title",
    labels={'ML_Experience_Midpoint': 'ML Experience (Years)'}
)
fig.update_layout(xaxis_title="Role Title", yaxis_title="ML Experience (Years)", xaxis_tickangle=45)
fig.show()

# Violin plot: Programming Experience by Role Title
fig = px.violin(
    df_cleaned,
    x='Role_Title',
    y='Programming_Experience_Midpoint',
    box=True,
    points="all",
    color='Role_Title',
    title="Programming Experience Distribution by Role Title",
    labels={'Programming_Experience_Midpoint': 'Programming Experience (Years)'}
)
fig.update_layout(xaxis_title="Role Title", yaxis_title="Programming Experience (Years)", xaxis_tickangle=45)
fig.show()

# Calculate the mean ML and programming experience for each framework, rounding to 1 decimal place
frameworks = [col for col in df_cleaned.columns if col.startswith("Framework")]
framework_experience = {
    framework: [
        round(df_cleaned[df_cleaned[framework] == 1]['ML_Experience_Midpoint'].mean(), 1),
        round(df_cleaned[df_cleaned[framework] == 1]['Programming_Experience_Midpoint'].mean(), 1)
    ]
    for framework in frameworks
}
experience_df = pd.DataFrame(framework_experience, index=['ML_Experience', 'Programming_Experience']).T

# Heatmap for ML Experience
fig = ff.create_annotated_heatmap(
    z=[experience_df['ML_Experience']],
    x=experience_df.index.tolist(),
    y=['ML Experience (Years)'],
    colorscale='Viridis',
    annotation_text=[[f"{value:.1f}" for value in experience_df['ML_Experience']]]
)
fig.update_layout(
    title="Average Machine Learning Experience by Framework",
    xaxis_tickangle=45,
    title_x=0.5,
    title_y=0.9,
    margin=dict(t=200, b=100)  # Add padding to avoid overlap
)
fig.show()

# Heatmap for Programming Experience
fig = ff.create_annotated_heatmap(
    z=[experience_df['Programming_Experience']],
    x=experience_df.index.tolist(),
    y=['Programming Experience (Years)'],
    colorscale='Cividis',
    annotation_text=[[f"{value:.1f}" for value in experience_df['Programming_Experience']]]
)
fig.update_layout(
    title="Average Programming Experience by Framework",
    xaxis_tickangle=45,
    title_x=0.5,
    title_y=0.9,
    margin=dict(t=200, b=100)
)
fig.show()

# Box plot: ML Experience by Country
fig = px.box(
    df_cleaned,
    x='Country',
    y='ML_Experience_Midpoint',
    title="Machine Learning Experience by Country",
    points="all",
    labels={'ML_Experience_Midpoint': 'ML Experience (Years)'}
)
fig.update_layout(xaxis_title="Country", yaxis_title="ML Experience (Years)", xaxis_tickangle=45)
fig.show()

# Box plot: Programming Experience by Country
fig = px.box(
    df_cleaned,
    x='Country',
    y='Programming_Experience_Midpoint',
    title="Programming Experience by Country",
    points="all",
    labels={'Programming_Experience_Midpoint': 'Programming Experience (Years)'}
)
fig.update_layout(xaxis_title="Country", yaxis_title="Programming Experience (Years)", xaxis_tickangle=45)
fig.show()

# Generate a box plot to show salary distribution by company size category
fig = px.box(df_cleaned, x='Company_Size', y='Average_Salary',
             title='Salary Distribution by Company Size',
             labels={'Company_Size': 'Company Size (Midpoint)', 'Average_Salary': 'Average Salary'})

# Customize layout for readability
fig.update_layout(xaxis_title="Company Size (Midpoint)", yaxis_title="Average Salary")
fig.show()

# Create a copy to ensure original data is not modified
df_encoded = df_cleaned_salary.copy()

# Combine binary columns by calculating the mean
df_encoded['Language'] = df_encoded[[col for col in df_encoded.columns if col.startswith("Language")]].mean(axis=1)
df_encoded['Learning'] = df_encoded[[col for col in df_encoded.columns if col.startswith("Learning")]].mean(axis=1)
df_encoded['IDE'] = df_encoded[[col for col in df_encoded.columns if col.startswith("IDE")]].mean(axis=1)
df_encoded['Framework'] = df_encoded[[col for col in df_encoded.columns if col.startswith("Framework")]].mean(axis=1)
df_encoded['Algorithm'] = df_encoded[[col for col in df_encoded.columns if col.startswith("Algorithm")]].mean(axis=1)


# Select columns for correlation
# This includes combined metrics, midpoints, and salary information
columns_for_correlation = [
    'Language', 'Learning', 'IDE', 'Framework', 'Algorithm',
    'Age_Midpoint', 'Salary_Min', 'Salary_Max',
    'ML_Experience_Midpoint', 'Programming_Experience_Midpoint', 'Company_Size'
]

# Calculate the correlation matrix
correlation_matrix = df_encoded[columns_for_correlation].corr().round(2)

# Visualize the correlation matrix using a heatmap
fig = ff.create_annotated_heatmap(
    z=correlation_matrix.values,
    x=correlation_matrix.columns.tolist(),
    y=correlation_matrix.columns.tolist(),
    annotation_text=correlation_matrix.values,
    colorscale="Viridis"
)
fig.update_layout(
    title="Correlation Matrix",
    xaxis_tickangle=45,
    width=1000,
    height=1000
)
fig.show()

"""## 4. Modeling

**4.1 Salary Prediction**
"""

from sklearn.preprocessing import StandardScaler

# 1. Drop unnecessary columns
df_modeling = df_cleaned_salary.drop(columns=['Salary_Min', 'Salary_Max', 'Age_Category',
                                       'ML_Experience_Category', 'Programming_Experience_Category', 'Year'])

# 2. Encode categorical variables
df_modeling = pd.get_dummies(df_modeling, columns=['Role_Title', 'Country', 'Gender', 'Education'], drop_first=True)

# 3. Scale numerical features
scaler = StandardScaler()
numerical_features = ['Age_Midpoint', 'ML_Experience_Midpoint', 'Programming_Experience_Midpoint', 'Company_Size']
df_modeling[numerical_features] = scaler.fit_transform(df_modeling[numerical_features])

# Display the prepared dataset
df_modeling.info()

#Check which columns have 'Other' entries to drop them in order not to create noise in the model

columns_to_check = ['Language - Other', 'IDE - Other', 'Framework - Other','Algorithm - Other', 'Role_Title_Other', 'Country_I do not wish to disclose my location', 'Gender_Other','Country_Other']

#entry_counts = df_modeling[columns_to_check].sum()
#print(entry_counts)
df_modeling = df_modeling.drop(columns= columns_to_check)
df_modeling.sum()

#Using LazyPredict for benchmarking
!pip install lazypredict

from lazypredict.Supervised import LazyRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Reduce size of df
df_modeling = df_modeling.sample(frac=0.3, random_state=42)

# Define features and target
X = df_modeling.drop(columns=['Average_Salary'])
y = df_modeling['Average_Salary']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize and run Lazypredict for regression
regressor = LazyRegressor(verbose=0, ignore_warnings=True, random_state=42)
models, predictions = regressor.fit(X_train, X_test, y_train, y_test)

# Display the results
print(models)

from lightgbm import LGBMRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, -1],
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [31, 50, 70],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'subsample': [0.8, 1.0],
}

# Initialize the LGBMRegressor model
lgbm = LGBMRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(
    estimator=lgbm,
    param_grid=param_grid,
    scoring='r2',
    cv=3,
    verbose=2,
    n_jobs=-1
)

# Fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

# Retrieve the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best R² Score:", best_score)

# Use the best model to make predictions on the test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate the model performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nFinal Model Performance:")
print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"R²: {r2}")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define features and target
X = df_modeling.drop(columns=['Average_Salary'])
y = df_modeling['Average_Salary']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a function to evaluate models
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Model: {model.__class__.__name__}")
    print(f"MAE: {mae}")
    print(f"MSE: {mse}")
    print(f"R²: {r2}")
    print("-" * 30)

# Initialize and evaluate models
models = [
    LinearRegression(),
    RandomForestRegressor(random_state=42, n_estimators=100),
    XGBRegressor(random_state=42, n_estimators=100),
    SVR()
]

# Evaluate each model
for model in models:
    evaluate_model(model, X_train, X_test, y_train, y_test)

!pip uninstall -y scikit-learn xgboost

!pip install "scikit-learn==1.5.0" "xgboost==1.7.6"

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor

# Define the X and y
X = df_modeling.drop(columns=['Average_Salary'])
y = df_modeling['Average_Salary']

# Define the parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200, 300, 400],
    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
    'max_depth': [3, 4, 5, 6, 7],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.3, 0.5]
}

# Initialize the model
xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)

# Set up RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,
    scoring='neg_mean_absolute_error',
    cv=3,
    verbose=1,
    n_jobs=-1,
    random_state=42)

# Fit the model
random_search.fit(X, y)

# Print the best parameters and score
print("Best Parameters:", random_search.best_params_)
print("Best MAE:", -random_search.best_score_)

# Train the XGBoost model with optimal parameters
final_xgb_model = XGBRegressor(
    objective='reg:squarederror',
    subsample=1.0,
    n_estimators=300,
    max_depth=7,
    learning_rate=0.05,
    gamma=0.3,
    colsample_bytree=0.6,
    random_state=42
)

# Fit the model to the training data
final_xgb_model.fit(X, y)

# Predict on the training set for evaluation
y_pred = final_xgb_model.predict(X)

# Evaluate the final model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

final_mae = mean_absolute_error(y, y_pred)
final_mse = mean_squared_error(y, y_pred)
final_r2 = r2_score(y, y_pred)

print("Final XGBoost Model Performance:")
print(f"MAE: {final_mae}")
print(f"MSE: {final_mse}")
print(f"R²: {final_r2}")

import joblib

# Save salary prediction model
joblib.dump(final_xgb_model, "salary_model.pkl")

from google.colab import files
files.download("salary_model.pkl")

importances = final_xgb_model.feature_importances_
feature_names = X_train.columns
sorted_indices = importances.argsort()
sorted_features = [feature_names[i] for i in sorted_indices]
sorted_importances = importances[sorted_indices]

# Create a DataFrame for easier plotting with Plotly
importance_df = pd.DataFrame({'Feature': sorted_features, 'Importance': sorted_importances})

# Create the horizontal bar plot with Plotly
fig = px.bar(importance_df, x='Importance', y='Feature', orientation='h',
             title="Feature Importance for XGBoost Model",
             labels={'Importance': 'Feature Importance', 'Feature': ''})

# Customize the layout for better visualization
fig.update_layout(
    height=1200,
    yaxis=dict(title='', automargin=True)
)
fig.show()

"""**4.2 Role Prediction**"""

from sklearn.preprocessing import StandardScaler
# For role prediction, we do not need salary-related columns or categorical columns that provide redundant information
df_modeling_roles = df_cleaned.drop(columns=['Salary_Min', 'Salary_Max', 'Average_Salary', 'Age_Category',
                                             'ML_Experience_Category', 'Programming_Experience_Category', 'Year', 'Company_Size'])

# Convert 'Role_Title' to numeric target labels and encode categorical features like 'Country' and 'Gender'
df_modeling_roles = pd.get_dummies(df_modeling_roles, columns=['Country', 'Gender', 'Education'], drop_first=True)


# Label encode 'Role_Title' to use as the target variable in classification
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_modeling_roles['Role_Title'] = le.fit_transform(df_modeling_roles['Role_Title'])

# Step 3: Scale numerical features
# Define the numerical features to scale
numerical_features = ['Age_Midpoint', 'ML_Experience_Midpoint', 'Programming_Experience_Midpoint']

# Initialize the scaler
scaler = StandardScaler()
df_modeling_roles[numerical_features] = scaler.fit_transform(df_modeling_roles[numerical_features])

# Display the prepared dataset
df_modeling_roles.info()

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
# Create a summary DataFrame
df_summary = pd.DataFrame({
    'Column Name': df_modeling_roles.columns,
    'Non-Null Count': df_modeling_roles.notnull().sum().values,
    'Data Type': df_modeling_roles.dtypes.values
})

# Display the summary
print(df_summary)

from sklearn.model_selection import train_test_split
# Define features (X) and target (y)
X = df_modeling_roles.drop(columns=['Role_Title'])
y = df_modeling_roles['Role_Title']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score


# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=200),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier(),
    "Support Vector Classifier": SVC()
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\nModel: {name}")
    print(f"Accuracy: {model.score(X_test, y_test):.4f}")
    print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")
    print(f"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}")
    print(f"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}")
    print('-' * 30)

from lazypredict.Supervised import LazyClassifier
from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df_modeling_roles.drop(columns=['Role_Title'])
y = df_modeling_roles['Role_Title']

# Sample 20% of the dataset
df_sampled = df_modeling_roles.sample(frac=0.2, random_state=42)

# Split the sampled data into features and target
X_sampled = df_sampled.drop(columns=['Role_Title'])
y_sampled = df_sampled['Role_Title']

# Split the sampled dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42, stratify=y_sampled)

# Initialize LazyClassifier
lazy_clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)

# Fit models and display results
models, predictions = lazy_clf.fit(X_train, X_test, y_train, y_test)

# Show the top 10 models
print(models.head(10))

!pip uninstall -y scikit-learn
!pip install scikit-learn==1.5.2

!pip uninstall xgboost
!pip install xgboost==1.6.2y

import sklearn
import xgboost
print("scikit-learn version:", sklearn.__version__)
print("xgboost version:", xgboost.__version__)

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV, train_test_split
import numpy as np

# Sample 20% of the dataset (adjust `frac` as needed)
df_sampled = df_modeling_roles.sample(frac=0.2, random_state=42)

# Define features (X) and target (y) for the sampled dataset
X = df_sampled.drop(columns=['Role_Title'])
y = df_sampled['Role_Title']

# Split the sampled dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define a reduced parameter grid for faster tuning
param_dist = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': np.linspace(0.01, 0.1, 3),
    'colsample_bytree': [0.6, 1.0],
    'subsample': [0.8, 1.0],
    'gamma': [0, 0.1, 0.3]
}

# Initialize the XGBoost Classifier
xgb = XGBClassifier(random_state=42)

# Set up RandomizedSearchCV to sample from the parameter grid
random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist,
                                   n_iter=25, cv=3, scoring='f1_weighted',
                                   n_jobs=-1, verbose=2, random_state=42)

# Fit the model to the training data
random_search.fit(X_train, y_train)

# Retrieve and display the best parameters and the best F1 score
best

from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
from xgboost import XGBClassifier
# Initialize the XGBoost Classifier with the best parameters
final_xgb_model = XGBClassifier(
    n_estimators= 200,
    max_depth=5,
    learning_rate=0.1,
    colsample_bytree=0.6,
    subsample=0.8,
    gamma=0.1,
    random_state=42
)

# Fit the model on the training data
final_xgb_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = final_xgb_model.predict(X_test)

# Evaluate the model using accuracy, F1 score, precision, and recall
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print("Final Model Performance:")
print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Precision:", precision)
print("Recall:", recall)

import joblib

# Save salary prediction model
joblib.dump(final_xgb_model, "role_model.pkl")

from google.colab import files
files.download("role_model.pkl")

importances = final_xgb_model.feature_importances_
feature_names = X_train.columns
sorted_indices = importances.argsort()
sorted_features = [feature_names[i] for i in sorted_indices]
sorted_importances = importances[sorted_indices]

# Create a DataFrame for easier plotting with Plotly
importance_df = pd.DataFrame({'Feature': sorted_features, 'Importance': sorted_importances})

# Create the horizontal bar plot with Plotly
fig = px.bar(importance_df, x='Importance', y='Feature', orientation='h',
             title="Feature Importance for XGBoost Model",
             labels={'Importance': 'Feature Importance', 'Feature': ''})

# Customize the layout for better visualization
fig.update_layout(
    height=1200,
    yaxis=dict(title='', automargin=True)
)
fig.show()



from sklearn.metrics import confusion_matrix
import plotly.graph_objects as go
import numpy as np

# Generate predictions and confusion matrix
y_pred = final_xgb_model.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))

# Define the confusion matrix heatmap with Plotly
fig = go.Figure(data=go.Heatmap(
    z=cm,
    x=np.unique(y_test),
    y=np.unique(y_test),
    colorscale='Viridis',
    showscale=True,
    colorbar=dict(title="Count"),
    zmin=0,
    zmax=1000
))

# Update layout for labels
fig.update_layout(
    title="Confusion Matrix",
    xaxis_title="Predicted",
    yaxis_title="Actual",
    xaxis=dict(tickmode='array', tickvals=np.unique(y_test)),
    yaxis=dict(tickmode='array', tickvals=np.unique(y_test)),
    height=600,
    width=700
)

# Show the figure
fig.show()

from sklearn.model_selection import cross_val_score

scores = cross_val_score(final_xgb_model, X_train, y_train, cv=5, scoring='f1_weighted')
print("Cross-Validation F1 Scores:", scores)
print("Mean F1 Score:", scores.mean())